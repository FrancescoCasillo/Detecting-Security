{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries and models uploaded!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm as SVM\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, Dropout, Input\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "cfg=K.tf.compat.v1.ConfigProto()\n",
    "\n",
    "# LOADING THE DATASET\n",
    "data = pd.read_pickle(\".\\ClassifiedDataset\\OriginalDataFeatureExtracted.pkl\")\n",
    "\n",
    "# LOADING THE SPACY MODEL\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# LOADING THE GLOSSARY\n",
    "f = open(\"./Glossary/NISTIR 7298 Rev3.txt\", \"r\")\n",
    "glossary = []\n",
    "for x in f:\n",
    "  glossary.append(x.replace(\"\\n\", \"\"))\n",
    "\n",
    "# LOADING SECBERT\n",
    "from transformers import pipeline\n",
    "\n",
    "fill_mask_secbert = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"jackaduma/SecBERT\",\n",
    "    tokenizer=\"jackaduma/SecBERT\"\n",
    ")\n",
    "# torch.device(\"cuda\") if torch.cuda.is_available() else\n",
    "device =  torch.device(\"cpu\")\n",
    "tokenizer = fill_mask_secbert.tokenizer\n",
    "secbert = fill_mask_secbert.model\n",
    "secbert.to(device)\n",
    "print(\"Libraries and models uploaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# print(torch.cuda.memory_summary(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS TO CREATE DIFFERENT MODELS #################################################################################################################################\n",
    "def create_models_EntPosDep(xtrain, ytrain):\n",
    "    max_length= 558\n",
    "    array = pd.DataFrame()\n",
    "    array[\"EntPosDep\"] = None\n",
    "    count = 0\n",
    "    for row in xtrain[['Entities','Dependencies','Parts of Speech']].iterrows():\n",
    "        tmp = []\n",
    "        for value in row[1]['Entities']:\n",
    "            tmp.append(value)\n",
    "        for value in row[1]['Dependencies']:\n",
    "            tmp.append(value)\n",
    "        for value in row[1]['Parts of Speech']:\n",
    "            tmp.append(value)\n",
    "        array.at[count,\"EntPosDep\"] = tmp\n",
    "        count += 1\n",
    "    \n",
    "    #Logistic Regression on Entity, Pos, Dependency\n",
    "    logreg = LogisticRegression(random_state=0, max_iter=250)\n",
    "    logreg.fit(array[\"EntPosDep\"].to_list(), ytrain)\n",
    "    #--------------------------------------------------\n",
    "    \n",
    "    #Support Vector Machines on Entity, Pos, Dependency \n",
    "    svm = SVM.SVR()\n",
    "    svm.fit(array[\"EntPosDep\"].to_list(), ytrain)\n",
    "    #--------------------------------------------------\n",
    "    \n",
    "    #Gaussian Naive Bayes on Entity, Pos, Dependency\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(array[\"EntPosDep\"].to_list(), ytrain)\n",
    "    #--------------------------------------------------\n",
    "    \n",
    "    #K-Nearest Neighbors with k = 5(default) on Entity, Pos, Dependency\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(array[\"EntPosDep\"].to_list(), ytrain)\n",
    "    #--------------------------------------------------\n",
    "    \n",
    "    #Decision Tree on Entity, Pos, Dependency\n",
    "    dt = tree.DecisionTreeClassifier()\n",
    "    dt.fit(array[\"EntPosDep\"].to_list(), ytrain)\n",
    "    #--------------------------------------------------\n",
    "    \n",
    "    #Random Forest on Entity, Pos, Dependency\n",
    "    rfc = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    rfc.fit(array[\"EntPosDep\"].to_list(), ytrain)\n",
    "    #--------------------------------------------------\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x1 = pad_sequences(np.array(xtrain['Entities']))\n",
    "        x2 = pad_sequences(np.array(xtrain['Dependencies']))\n",
    "        x3 = pad_sequences(np.array(xtrain['Parts of Speech']))\n",
    "        #CNN on Entity, Pos, Dependency\n",
    "        vocab_size = int(len(tokenizer.get_vocab())/1000 + 1)\n",
    "        np.random.seed(7)\n",
    "\n",
    "        # channel 1\n",
    "        inputs1 = Input(shape=(max_length,))\n",
    "        embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "        conv1 = Conv1D(filters=16, kernel_size=4, activation='relu')(embedding1)\n",
    "        drop1 = Dropout(0.2)(conv1)\n",
    "        pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "        flat1 = Flatten()(pool1)\n",
    "\n",
    "        # channel 2\n",
    "        inputs2 = Input(shape=(max_length,))\n",
    "        embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "        conv2 = Conv1D(filters=16, kernel_size=4, activation='relu')(embedding2)\n",
    "        drop2 = Dropout(0.2)(conv2)\n",
    "        pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "        flat2 = Flatten()(pool2)\n",
    "        \n",
    "        # channel 3\n",
    "        inputs3 = Input(shape=(max_length,))\n",
    "        embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "        conv3 = Conv1D(filters=16, kernel_size=4, activation='relu')(embedding3)\n",
    "        drop3 = Dropout(0.2)(conv3)\n",
    "        pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "        flat3 = Flatten()(pool3)\n",
    "\n",
    "        # merge\n",
    "        merged = concatenate([flat1, flat2, flat3])\n",
    "        # interpretation\n",
    "        dense1 = Dense(500, activation='relu')(merged)\n",
    "        dense2 = Dense(50, activation='relu')(dense1)\n",
    "        dense3 = Dense(5, activation='relu')(dense2)\n",
    "        outputs = Dense(1, activation='sigmoid')(dense3)\n",
    "\n",
    "        cnn = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "        # compile\n",
    "        cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        cnn.fit([x1,x2,x3], np.array(ytrain), epochs=20, batch_size=50)\n",
    "    #--------------------------------------------------\n",
    "    \n",
    "    return logreg, svm, gnb, knn, dt, rfc, cnn\n",
    "\n",
    "\n",
    "def create_model_SecWorSimReq(xtrain,ytrain):\n",
    "    max_length= 558\n",
    "\n",
    "    array = pd.DataFrame()\n",
    "    array[\"SecWords&SimReq\"] = None\n",
    "    count = 0\n",
    "    for row in xtrain[['Security Words','Similar Requirements']].iterrows():\n",
    "        tmp = []\n",
    "        for value in row[1]['Security Words']:\n",
    "            tmp.append(value)\n",
    "        for value in row[1]['Similar Requirements']:\n",
    "            tmp.append(value)\n",
    "        array.at[count,\"SecWords&SimReq\"] = tmp\n",
    "        count += 1\n",
    "\n",
    "    #Logistic Regression on Security Words and Similar Requirements \n",
    "    logsw = LogisticRegression(random_state=0, max_iter=250)\n",
    "    logsw.fit(array[\"SecWords&SimReq\"].to_list(), ytrain)\n",
    "    #--------------------------------------------------\n",
    "    #Support Vector Machines on Security Words and Similar Requirements\n",
    "    svmsw = SVM.SVR()\n",
    "    svmsw.fit(array[\"SecWords&SimReq\"].to_list(), ytrain)\n",
    "    #--------------------------------------------------\n",
    "    \n",
    "    #Gaussian Naive Bayes on Security Words and Similar Requirements\n",
    "    gnbsw = GaussianNB()\n",
    "    gnbsw.fit(array[\"SecWords&SimReq\"].to_list(), ytrain)\n",
    "    #--------------------------------------------------\n",
    "    \n",
    "    #K-Nearest Neighbors with k = 5(default)\n",
    "    knnsw = KNeighborsClassifier()\n",
    "    knnsw.fit(array[\"SecWords&SimReq\"].to_list(), ytrain)\n",
    "    #--------------------------------------------------\n",
    "    \n",
    "    #Decision Tree on Security Words and Similar Requirements\n",
    "    dtsw = tree.DecisionTreeClassifier()\n",
    "    dtsw.fit(array[\"SecWords&SimReq\"].to_list(), ytrain)\n",
    "    #--------------------------------------------------\n",
    "    \n",
    "    #Random Forest on Security Words and Similar Requirements\n",
    "    rfcsw = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    rfcsw.fit(array[\"SecWords&SimReq\"].to_list(), ytrain)\n",
    "    #--------------------------------------------------\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x1 = pad_sequences(np.array(xtrain['Security Words']))\n",
    "        x2 = pad_sequences(np.array(xtrain['Similar Requirements']))\n",
    "        #CNN on Security Words and Similar Requirements\n",
    "        vocab_size = int(len(tokenizer.get_vocab())/1000 + 1)\n",
    "        np.random.seed(7)\n",
    "\n",
    "        # channel 1\n",
    "        inputs1 = Input(shape=(max_length,))\n",
    "        embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "        conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "        drop1 = Dropout(0.2)(conv1)\n",
    "        pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "        flat1 = Flatten()(pool1)\n",
    "\n",
    "        # channel 2\n",
    "        inputs2 = Input(shape=(max_length,))\n",
    "        embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "        conv2 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding2)\n",
    "        drop2 = Dropout(0.2)(conv2)\n",
    "        pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "        flat2 = Flatten()(pool2)\n",
    "\n",
    "        # merge\n",
    "        merged = concatenate([flat1, flat2])\n",
    "        # interpretation\n",
    "        dense1 = Dense(500, activation='relu')(merged)\n",
    "        dense2 = Dense(50, activation='relu')(dense1)\n",
    "        dense3 = Dense(5, activation='relu')(dense2)\n",
    "        outputs = Dense(1, activation='sigmoid')(dense3)\n",
    "\n",
    "        cnn_sw = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "        # compile\n",
    "        cnn_sw.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        cnn_sw.fit([x1,x2], np.array(ytrain), epochs=20, batch_size=50)\n",
    "        #--------------------------------------------------\n",
    "    \n",
    "    return logsw, svmsw, gnbsw, knnsw, dtsw, rfcsw, cnn_sw\n",
    "    \n",
    "\n",
    "\n",
    "def create_TL_model(x_train, y_train):\n",
    "    #Padding the test set for CNNs\n",
    "    x1 = pad_sequences(np.array(x_train[\"Entities\"]))\n",
    "    x2 = pad_sequences(np.array(x_train[\"Dependencies\"]))\n",
    "    x3 = pad_sequences(np.array(x_train[\"Parts of Speech\"]))\n",
    "    x4 = pad_sequences(np.array(x_train[\"Secbert Outputs\"]))\n",
    "    K.clear_session()\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        max_length= 558\n",
    "        vocab_size = int(len(tokenizer.get_vocab())/1000 + 1)\n",
    "        \n",
    "        #lenght of input for TL coming from the output of BERT\n",
    "        input_shape = 33280 #max_dim = 126, to be sure we set as max_dim = 130, then multiply it for the lenght of each one (768) = 99840, taking element each 5 value( the more elements are taken, the more accurate the modell will be, but depends on the memory of the machine) = 19968\n",
    "        \n",
    "        #Transfer Learning Model\n",
    "        np.random.seed(7)\n",
    "\n",
    "        # channel 1\n",
    "        inputs1 = Input(shape=(max_length,))\n",
    "        embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "        conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "        drop1 = Dropout(0.2)(conv1)\n",
    "        pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "        flat1 = Flatten()(pool1)\n",
    "\n",
    "        # channel 2\n",
    "        inputs2 = Input(shape=(max_length,))\n",
    "        embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "        conv2 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding2)\n",
    "        drop2 = Dropout(0.2)(conv2)\n",
    "        pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "        flat2 = Flatten()(pool2)\n",
    "\n",
    "        # channel 3\n",
    "        inputs3 = Input(shape=(max_length,))\n",
    "        embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "        conv3 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding3)\n",
    "        drop3 = Dropout(0.2)(conv3)\n",
    "        pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "        flat3 = Flatten()(pool3)\n",
    "\n",
    "        #output of secbert model\n",
    "        inputs4 = Input(shape=(input_shape,))\n",
    "        # embedding4 = Embedding(vocab_size, 100)(inputs4)\n",
    "        # conv4 = Conv1D(filters=16, kernel_size=2, activation='relu')(embedding4)\n",
    "        # drop4 = Dropout(0.5)(conv4)\n",
    "        # pool4 = MaxPooling1D(pool_size=4)(drop4)\n",
    "        # drop4 = Dropout(0.5)(pool4)\n",
    "        # pool4 = MaxPooling1D(pool_size=4)(drop4)\n",
    "        # drop4 = Dropout(0.5)(pool4)\n",
    "        # pool4 = MaxPooling1D(pool_size=4)(drop4)\n",
    "        # flat4 = Flatten()(pool4)\n",
    "\n",
    "        # merge flat1, flat2, flat3,\n",
    "        merged = concatenate([ flat1, flat2, flat3, inputs4])\n",
    "        # interpretation\n",
    "        dense = Dense(500, activation='relu')(merged)\n",
    "        dense1 = Dense(50, activation='relu')(dense)\n",
    "        dense2 = Dense(5, activation='relu')(dense1)\n",
    "        # dense3 = Dense(1, activation='relu')(dense2)\n",
    "        outputs = Dense(1, activation='sigmoid')(dense2)\n",
    "\n",
    "        tl_cnn = Model(inputs=[inputs1,inputs2,inputs3,inputs4], outputs=outputs)\n",
    "        # compile\n",
    "        tl_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        tl_cnn.fit([x1,x2,x3,x4], np.array(y_train), epochs=20, batch_size=20, use_multiprocessing=False)\n",
    "        #--------------------------------------------------\n",
    "        \n",
    "        return tl_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_EntPosDep = create_models_EntPosDep()\n",
    "# print(models_EntPosDep)\n",
    "# del models_EntPosDep\n",
    "# models_SecWorSimReq = create_model_SecWorSimReq()\n",
    "# print(models_SecWorSimReq)\n",
    "# del models_SecWorSimReq\n",
    "# model_tl = create_TL_model()\n",
    "# print(model_tl)\n",
    "# del model_tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross-Validation\n",
    "# def cross_validation(model, _X, _y, _cv=5):\n",
    "#       '''Function to perform k Folds Cross-Validation\n",
    "#        Parameters\n",
    "#        ----------\n",
    "#       model: Python Class, default=None\n",
    "#               This is the machine learning algorithm to be used for training.\n",
    "#       _X: array\n",
    "#            This is the matrix of features.\n",
    "#       _y: array\n",
    "#            This is the target variable.\n",
    "#       _cv: int, default=5\n",
    "#           Determines the number of folds for cross-validation.\n",
    "#        Returns\n",
    "#        -------\n",
    "#        The function returns a dictionary containing the metrics 'accuracy', 'precision',\n",
    "#        'recall', 'f1' for both training set and validation set.\n",
    "#       '''\n",
    "#       _scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "#       results = cross_validate(estimator=model,\n",
    "#                                X=_X,\n",
    "#                                y=_y,\n",
    "#                                cv=_cv,\n",
    "#                                scoring=_scoring,\n",
    "#                                return_train_score=True)\n",
    "      \n",
    "#       return {\"Training Accuracy scores\": results['train_accuracy'],\n",
    "#               \"Mean Training Accuracy\": results['train_accuracy'].mean()*100,\n",
    "#               \"Training Precision scores\": results['train_precision'],\n",
    "#               \"Mean Training Precision\": results['train_precision'].mean(),\n",
    "#               \"Training Recall scores\": results['train_recall'],\n",
    "#               \"Mean Training Recall\": results['train_recall'].mean(),\n",
    "#               \"Training F1 scores\": results['train_f1'],\n",
    "#               \"Mean Training F1 Score\": results['train_f1'].mean(),\n",
    "#               \"Validation Accuracy scores\": results['test_accuracy'],\n",
    "#               \"Mean Validation Accuracy\": results['test_accuracy'].mean()*100,\n",
    "#               \"Validation Precision scores\": results['test_precision'],\n",
    "#               \"Mean Validation Precision\": results['test_precision'].mean(),\n",
    "#               \"Validation Recall scores\": results['test_recall'],\n",
    "#               \"Mean Validation Recall\": results['test_recall'].mean(),\n",
    "#               \"Validation F1 scores\": results['test_f1'],\n",
    "#               \"Mean Validation F1 Score\": results['test_f1'].mean()\n",
    "#               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model in create_models_EntPosDep():\n",
    "#     print(\"Cross validation on \"+ str(model))\n",
    "#     # _scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "#     feature_cols = ['Entities', 'Dependencies','Parts of Speech']\n",
    "#     x = data.loc[:, feature_cols].astype(float)\n",
    "#     dep_var = [\"Security Related\"]\n",
    "#     y = data.loc[:, dep_var]\n",
    "#     results = cross_val_score(model, X = x[:1000], y = y[:1000], cv=2)\n",
    "#     print(results)\n",
    "    # results = cross_val_score(estimator=models_EntPosDep[0],\n",
    "    #                             X=x[:1],\n",
    "    #                             y=y[:1],\n",
    "    #                             cv=1,\n",
    "    #                             scoring=_scoring,\n",
    "    #                             return_train_score=True)\n",
    "    # cross_validation(model,data[[\"Entities_enc\",\"Dependencies_enc\",\"Parts of Speech_enc\"]],data[[\"Security Related\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model in create_models_EntPosDep()[:5]:\n",
    "#     print(\"Cross validation on \"+ str(model))\n",
    "#     feature_cols = ['Entities', 'Dependencies','Parts of Speech']\n",
    "#     x = data.loc[:, feature_cols].astype(float)\n",
    "#     dep_var = [\"Security Related\"]\n",
    "#     y = data.loc[:, dep_var]\n",
    "#     results = cross_validate(estimator=model,\n",
    "#                                X=x,\n",
    "#                                y=y,\n",
    "#                                cv=1,\n",
    "#                                scoring=_scoring,\n",
    "#                                return_train_score=True)\n",
    "\n",
    "\n",
    "\n",
    "# for model in create_model_SecWorSimReq()[:5]:\n",
    "#     print(\"Cross validation on \"+ str(model))\n",
    "#     feature_cols = ['Security Words','Similar Requirements']\n",
    "#     x = data.loc[:, feature_cols].astype(float)\n",
    "#     dep_var = [\"Security Related\"]\n",
    "#     y = data.loc[:, dep_var]\n",
    "#     results = cross_validate(estimator=model,\n",
    "#                                X=x,\n",
    "#                                y=y,\n",
    "#                                cv=1,\n",
    "#                                scoring=_scoring,\n",
    "#                                return_train_score=True)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = create_models_EntPosDep()[0]\n",
    "# print(\"Cross validation on \"+ str(model))\n",
    "#     # _scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "# feature_cols = ['Entities', 'Dependencies','Parts of Speech']\n",
    "# print(\"Slicing ...\")\n",
    "# x = data.loc[:150, feature_cols]\n",
    "# temp=np.concatenate((x['Entities'][:100],x['Dependencies'][:100]),axis=0)\n",
    "# xtrain= np.concatenate((temp,x['Parts of Speech'][:100]),axis=0)\n",
    "# dep_var = [\"Security Related\"]\n",
    "# y = data.loc[:150, dep_var]\n",
    "# print(\"Fitting \"+ str(model))\n",
    "# model.fit(xtrain, y[:100])\n",
    "# temp=np.concatenate((x['Entities'][100:150],x['Dependencies'][100:150]),axis=0)\n",
    "# xtest= np.concatenate((temp,x['Parts of Speech'][100:150]),axis=0)\n",
    "# y_pred = model.predict(xtest).round()\n",
    "# print(f1_score(y[100:150], y_pred))\n",
    "# print(precision_score(y[100:150], y_pred))\n",
    "# print(recall_score(y[100:150], y_pred))\n",
    "# print(accuracy_score(y[100:150], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fold_1(data):\n",
    "    # CREATING FIRST FOLD\n",
    "    xtrain = pd.concat( [ data[data[\"Security Related\"]==True][928:], data[data[\"Security Related\"]==False][1247:] ] )\n",
    "    ytrain = np.array([1 for _ in range(len(xtrain[xtrain['Security Related']==True]))] + [0 for _ in range((len(xtrain[xtrain['Security Related']==False])))])\n",
    "\n",
    "    xtest = pd.concat( [ data[data[\"Security Related\"]==True][:928], data[data[\"Security Related\"]==False][:1247] ] )\n",
    "    ytest = np.array([1 for _ in range(len(xtest[xtest['Security Related']==True]))] + [0 for _ in range((len(xtest[xtest['Security Related']==False])))])\n",
    "\n",
    "    return xtrain, xtest, ytrain, ytest\n",
    "\n",
    "def create_fold_2(data):\n",
    "    # CREATING SECOND FOLD\n",
    "    xtrain = pd.concat( [ data[data[\"Security Related\"]==True][:928],data[data[\"Security Related\"]==True][1856:] ] )\n",
    "    xtrain = pd.concat( [ xtrain, data[data[\"Security Related\"]==False][:1247] ] )\n",
    "    xtrain = pd.concat( [ xtrain, data[data[\"Security Related\"]==False][2493:] ] )\n",
    "    ytrain = np.array([1 for _ in range(len(xtrain[xtrain['Security Related']==True]))] + [0 for _ in range((len(xtrain[xtrain['Security Related']==False])))])\n",
    "\n",
    "    xtest = pd.concat( [ data[data[\"Security Related\"]==True][928:1856], data[data[\"Security Related\"]==False][1247:2493] ] )\n",
    "    ytest = np.array([1 for _ in range(len(xtest[xtest['Security Related']==True]))] + [0 for _ in range((len(xtest[xtest['Security Related']==False])))])\n",
    "\n",
    "    return xtrain, xtest, ytrain, ytest\n",
    "#--------------------------------------------------\n",
    "def create_fold_3(data):\n",
    "    # CREATING THIRD FOLD\n",
    "    xtrain = pd.concat( [ data[data[\"Security Related\"]==True][:1856], data[data[\"Security Related\"]==True][2784:] ] )\n",
    "    xtrain = pd.concat( [ xtrain, data[data[\"Security Related\"]==False][:2493] ] )\n",
    "    xtrain = pd.concat( [ xtrain, data[data[\"Security Related\"]==False][3739:] ] )\n",
    "    ytrain = np.array([1 for _ in range(len(xtrain[xtrain['Security Related']==True]))] + [0 for _ in range((len(xtrain[xtrain['Security Related']==False])))])\n",
    "\n",
    "    xtest = pd.concat( [ data[data[\"Security Related\"]==True][1856:2784], data[data[\"Security Related\"]==False][2493:3739] ] )\n",
    "    ytest = np.array([1 for _ in range(len(xtest[xtest['Security Related']==True]))] + [0 for _ in range((len(xtest[xtest['Security Related']==False])))])\n",
    "\n",
    "    return xtrain, xtest, ytrain, ytest\n",
    "\n",
    "def create_fold_4(data):\n",
    "    # CREATING FOURTH FOLD\n",
    "    xtrain = pd.concat( [ data[data[\"Security Related\"]==True][:2784], data[data[\"Security Related\"]==True][3712:] ] )\n",
    "    xtrain = pd.concat( [ xtrain, data[data[\"Security Related\"]==False][:3739] ] )\n",
    "    xtrain = pd.concat( [ xtrain, data[data[\"Security Related\"]==False][4985:] ] )\n",
    "    ytrain = np.array([1 for _ in range(len(xtrain[xtrain['Security Related']==True]))] + [0 for _ in range((len(xtrain[xtrain['Security Related']==False])))])\n",
    "\n",
    "    xtest = pd.concat( [ data[data[\"Security Related\"]==True][2784:3712], data[data[\"Security Related\"]==False][3739:4985] ] )\n",
    "    ytest = np.array([1 for _ in range(len(xtest[xtest['Security Related']==True]))] + [0 for _ in range((len(xtest[xtest['Security Related']==False])))])\n",
    "\n",
    "    return xtrain, xtest, ytrain, ytest\n",
    "\n",
    "def create_fold_5(data):\n",
    "    # CREATING FIFTH FOLD\n",
    "    xtrain = pd.concat( [ data[data[\"Security Related\"]==True][:3712], data[data[\"Security Related\"]==False][:4985] ] )\n",
    "    ytrain = np.array([1 for _ in range(len(xtrain[xtrain['Security Related']==True]))] + [0 for _ in range((len(xtrain[xtrain['Security Related']==False])))])\n",
    "\n",
    "    xtest = pd.concat( [ data[data[\"Security Related\"]==True][3712:], data[data[\"Security Related\"]==False][4985:] ] )\n",
    "    ytest = np.array([1 for _ in range(len(xtest[xtest['Security Related']==True]))] + [0 for _ in range((len(xtest[xtest['Security Related']==False])))])\n",
    "\n",
    "    return xtrain, xtest, ytrain, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xtrain, ytrain, xtest, ytest = create_fold_1(data)\n",
    "# print(\"Creating \")\n",
    "# #Logistic Regression on Security Words and Similar Requirements \n",
    "# logsw = LogisticRegression(random_state=0)\n",
    "# print(\"Fitting \")\n",
    "# array = pd.DataFrame()\n",
    "# array[\"SecWords&SimReq\"] = None\n",
    "# count = 0\n",
    "# for row in xtrain[['Security Words','Similar Requirements']].iterrows():\n",
    "#     tmp = []\n",
    "#     for value in row[1]['Security Words']:\n",
    "#         tmp.append(value)\n",
    "#     for value in row[1]['Similar Requirements']:\n",
    "#         tmp.append(value)\n",
    "#     array.at[count,\"SecWords&SimReq\"] = tmp\n",
    "#     count += 1\n",
    "# # xtrain[['Security Words','Similar Requirements']][:10]\n",
    "# # ytrain[['Security Related']][:1].values.ravel()\n",
    "# logsw.fit(array[\"SecWords&SimReq\"].to_list(),ytrain['Security Related'].to_list())\n",
    "# logsw.predict(array[\"SecWords&SimReq\"].to_list())\n",
    "\n",
    "# logsw.fit([[1000,1500],[2000,300]],[1,0])\n",
    "# logsw.predict([[2000,1000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xtrain, xtest,ytrain, ytest = create_fold_1(data)\n",
    "# create_TL_model(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(resultsfold, logreg, logsw, svm, svmsw, gnb, gnbsw, knn, knnsw, dt, dtsw, rfc, rfcsw, sec_words_cnn, cnn, xtest, y_test):\n",
    "    \n",
    "    counter=0\n",
    "    resultsfold.loc[counter,\"Real Prediction\"] = \"\"\n",
    "    for i in y_test:\n",
    "        resultsfold.loc[counter,\"Real Prediction\"] = i\n",
    "        counter+=1\n",
    "\n",
    "    #Preparing input for shallow models\n",
    "    array = pd.DataFrame()\n",
    "    array[\"EntPosDep\"] = None\n",
    "    count = 0\n",
    "    for row in xtest[['Entities','Dependencies','Parts of Speech']].iterrows():\n",
    "        tmp = []\n",
    "        for value in row[1]['Entities']:\n",
    "            tmp.append(value)\n",
    "        for value in row[1]['Dependencies']:\n",
    "            tmp.append(value)\n",
    "        for value in row[1]['Parts of Speech']:\n",
    "            tmp.append(value)\n",
    "        array.at[count,\"EntPosDep\"] = tmp\n",
    "        count += 1\n",
    "    array[\"SecWords&SimReq\"] = None\n",
    "    count = 0\n",
    "    for row in xtest[['Security Words','Similar Requirements']].iterrows():\n",
    "        tmp = []\n",
    "        for value in row[1]['Security Words']:\n",
    "            tmp.append(value)\n",
    "        for value in row[1]['Similar Requirements']:\n",
    "            tmp.append(value)\n",
    "        array.at[count,\"SecWords&SimReq\"] = tmp\n",
    "        count += 1\n",
    "    \n",
    "    #Padding the test set for CNNs\n",
    "    x1 = pad_sequences(np.array(xtest[\"Entities\"]))\n",
    "    x2 = pad_sequences(np.array(xtest[\"Dependencies\"]))\n",
    "    x3 = pad_sequences(np.array(xtest[\"Parts of Speech\"]))\n",
    "    x4 = pad_sequences(np.array(xtest['Security Words']))\n",
    "    x5 = pad_sequences(np.array(xtest['Similar Requirements']))\n",
    "    \n",
    "    #------------------------------------------------------\n",
    "    #Result of Logistic Regression on EntPosDep\n",
    "    y_pred = logreg.predict(array[\"EntPosDep\"].to_list()).round()\n",
    "    \n",
    "    counter=0\n",
    "    resultsfold.loc[counter,\"LR\"] = \"\"\n",
    "    for i in y_pred:\n",
    "        resultsfold.loc[counter,\"LR\"] = i\n",
    "        counter+=1\n",
    "    \n",
    "    f1_log = f1_score(y_test, y_pred)\n",
    "      \n",
    "    acc_log = accuracy_score(y_test, y_pred)\n",
    "    #------------------------------------------------------\n",
    "    \n",
    "    #Result of Logstic Regression on sec Words\n",
    "    y_pred = logsw.predict(array[\"SecWords&SimReq\"].to_list()).round()\n",
    "    \n",
    "    counter=0\n",
    "    resultsfold.loc[counter,\"LRsw\"] = \"\"\n",
    "    for i in y_pred:\n",
    "        resultsfold.loc[counter,\"LRsw\"] = i\n",
    "        counter+=1\n",
    "    \n",
    "    f1_logsw = f1_score(y_test, y_pred)\n",
    "        \n",
    "    acc_logsw = accuracy_score(y_test, y_pred)\n",
    "    #------------------------------------------------------\n",
    "    \n",
    "    #Result of Support Vector Machines on EntPosDep\n",
    "    y_pred = svm.predict(array[\"EntPosDep\"].to_list()).round()\n",
    "    \n",
    "    counter=0\n",
    "    resultsfold.loc[counter,\"SVM\"] = \"\"\n",
    "    for i in y_pred:\n",
    "        resultsfold.loc[counter,\"SVM\"] = i\n",
    "        counter+=1\n",
    "    \n",
    "    f1_svm = f1_score(y_test, y_pred)\n",
    "      \n",
    "    acc_svm = accuracy_score(y_test, y_pred)\n",
    "    #------------------------------------------------------\n",
    "    \n",
    "    #Result of Support Vector Machines on sec Words\n",
    "    y_pred = svmsw.predict(array[\"SecWords&SimReq\"].to_list()).round()\n",
    "    counter=0\n",
    "    resultsfold.loc[counter,\"SVMsw\"] = \"\"\n",
    "    for i in y_pred:\n",
    "        resultsfold.loc[counter,\"SVMsw\"] = i\n",
    "        counter+=1\n",
    "    \n",
    "    f1_svmsw = f1_score(y_test, abs(y_pred))\n",
    "        \n",
    "    acc_svmsw = accuracy_score(y_test, y_pred)\n",
    "    #------------------------------------------------------\n",
    "    \n",
    "    #Result of Gaussian Naive Bayes on EntPosDep\n",
    "    y_pred = gnb.predict(array[\"EntPosDep\"].to_list()).round()\n",
    "    \n",
    "    counter=0\n",
    "    resultsfold.loc[counter,\"GNB\"] = \"\"\n",
    "    for i in y_pred:\n",
    "        resultsfold.loc[counter,\"GNB\"] = i\n",
    "        counter+=1\n",
    "    \n",
    "    f1_gnb = f1_score(y_test, y_pred)\n",
    "    # f1_gnb = sum(f1_gnb)/len(f1_gnb)\n",
    "      \n",
    "    acc_gnb = accuracy_score(y_test, y_pred)\n",
    "    #------------------------------------------------------\n",
    "    \n",
    "    #Result of Gaussian Naive Bayes on sec Words\n",
    "    y_pred = gnbsw.predict(array[\"SecWords&SimReq\"].to_list()).round()\n",
    "    \n",
    "    counter=0\n",
    "    resultsfold.loc[counter,\"GNBsw\"] = \"\"\n",
    "    for i in y_pred:\n",
    "        resultsfold.loc[counter,\"GNBsw\"] = i\n",
    "        counter+=1\n",
    "     \n",
    "    f1_gnbsw = f1_score(y_test, y_pred)\n",
    "    # f1_gnbsw = sum(f1_gnbsw)/len(f1_gnbsw)\n",
    "        \n",
    "    acc_gnbsw = accuracy_score(y_test, y_pred)\n",
    "    #------------------------------------------------------\n",
    "    \n",
    "    #Result of k-Nearest Neighbors on EntPosDep\n",
    "    y_pred = knn.predict(array[\"EntPosDep\"].to_list()).round()\n",
    "    \n",
    "    counter=0\n",
    "    resultsfold.loc[counter,\"KNN\"] = \"\"\n",
    "    for i in y_pred:\n",
    "        resultsfold.loc[counter,\"KNN\"] = i\n",
    "        counter+=1\n",
    "    \n",
    "    f1_knn = f1_score(y_test, y_pred)\n",
    "    # f1_knn = sum(f1_knn)/len(f1_knn)\n",
    "      \n",
    "    acc_knn = accuracy_score(y_test, y_pred)\n",
    "    #------------------------------------------------------\n",
    "    \n",
    "    #Result of k-Nearest Neighbors on sec Words\n",
    "    y_pred = knnsw.predict(array[\"SecWords&SimReq\"].to_list()).round()\n",
    "    \n",
    "    counter=0\n",
    "    resultsfold.loc[counter,\"KNNsw\"] = \"\"\n",
    "    for i in y_pred:\n",
    "        resultsfold.loc[counter,\"KNNsw\"] = i\n",
    "        counter+=1\n",
    "    \n",
    "    f1_knnsw = f1_score(y_test, y_pred)\n",
    "    # f1_knnsw = sum(f1_knnsw)/len(f1_knnsw)\n",
    "        \n",
    "    acc_knnsw = accuracy_score(y_test, y_pred)\n",
    "    #------------------------------------------------------\n",
    "    \n",
    "    #Result of Decision Tree on EntPosDep\n",
    "    y_pred = dt.predict(array[\"EntPosDep\"].to_list()).round()\n",
    "    \n",
    "    counter=0\n",
    "    resultsfold.loc[counter,\"DT\"] = \"\"\n",
    "    for i in y_pred:\n",
    "        resultsfold.loc[counter,\"DT\"] = i\n",
    "        counter+=1\n",
    "    \n",
    "    f1_dt = f1_score(y_test, y_pred)\n",
    "    # f1_dt = sum(f1_dt)/len(f1_dt)\n",
    "     \n",
    "    acc_dt = accuracy_score(y_test, y_pred)\n",
    "    #------------------------------------------------------\n",
    "    \n",
    "    #Result of Decision Tree on sec Words\n",
    "    y_pred = dtsw.predict(array[\"SecWords&SimReq\"].to_list()).round()\n",
    "    \n",
    "    counter=0\n",
    "    resultsfold.loc[counter,\"DTsw\"] = \"\"\n",
    "    for i in y_pred:\n",
    "        resultsfold.loc[counter,\"DTsw\"] = i\n",
    "        counter+=1\n",
    "     \n",
    "    f1_dtsw = f1_score(y_test, y_pred)\n",
    "    # f1_dtsw = sum(f1_dtsw)/len(f1_dtsw)\n",
    "       \n",
    "    acc_dtsw = accuracy_score(y_test, y_pred)\n",
    "    #------------------------------------------------------\n",
    "    \n",
    "    #Result of Random Forest Classifier on EntPosDep\n",
    "    y_pred = rfc.predict(array[\"EntPosDep\"].to_list()).round()\n",
    "    \n",
    "    counter=0\n",
    "    resultsfold.loc[counter,\"RFC\"] = \"\"\n",
    "    for i in y_pred:\n",
    "        resultsfold.loc[counter,\"RFC\"] = i\n",
    "        counter+=1\n",
    "    \n",
    "    f1_rfc = f1_score(y_test, y_pred)\n",
    "    # f1_rfc = sum(f1_rfc)/len(f1_rfc)\n",
    "      \n",
    "    acc_rfc = accuracy_score(y_test, y_pred)\n",
    "    #------------------------------------------------------\n",
    "    \n",
    "    #Result of Random Forest Classifier on sec Words\n",
    "    y_pred = rfcsw.predict(array[\"SecWords&SimReq\"].to_list()).round()\n",
    "    \n",
    "    counter=0\n",
    "    resultsfold.loc[counter,\"RFCsw\"] = \"\"\n",
    "    for i in y_pred:\n",
    "        resultsfold.loc[counter,\"RFCsw\"] = i\n",
    "        counter+=1\n",
    "    \n",
    "    f1_rfcsw = f1_score(y_test, y_pred)\n",
    "    # f1_rfcsw = sum(f1_rfcsw)/len(f1_rfcsw)\n",
    "        \n",
    "    acc_rfcsw = accuracy_score(y_test, y_pred)\n",
    "    #------------------------------------------------------\n",
    "    \n",
    "    #Result of Security Words CNN\n",
    "    with torch.no_grad():\n",
    "        y_pred = sec_words_cnn.predict([x4,x5]).round()\n",
    "    \n",
    "    counter=0\n",
    "    resultsfold.loc[counter,\"sw_CNN\"] = \"\"\n",
    "    for i in y_pred:\n",
    "        resultsfold.loc[counter,\"sw_CNN\"] = i\n",
    "        counter+=1\n",
    "    \n",
    "    f1_sw_cnn = f1_score(y_test, y_pred)\n",
    "    # f1_sw_cnn = sum(f1_sw_cnn)/len(f1_sw_cnn)\n",
    "    \n",
    "    \n",
    "    acc_sw_cnn = accuracy_score(y_test, y_pred)\n",
    "    #------------------------------------------------------\n",
    "    \n",
    "    #Result of Disclosure CNN\n",
    "    with torch.no_grad():\n",
    "        y_pred = cnn.predict([x1,x2,x3]).round()\n",
    "    \n",
    "    counter=0\n",
    "    resultsfold.loc[counter,\"CNN\"] = \"\"\n",
    "    for i in y_pred:\n",
    "        resultsfold.loc[counter,\"CNN\"] = i\n",
    "        counter+=1\n",
    "    \n",
    "    f1_cnn = f1_score(y_test, y_pred)\n",
    "    # f1_cnn = sum(f1_cnn)/len(f1_cnn)\n",
    "      \n",
    "    acc_cnn = accuracy_score(y_test, y_pred)\n",
    "    #------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    return resultsfold, f1_log, acc_log, f1_logsw, acc_logsw, f1_svm, acc_svm, f1_svmsw, acc_svmsw, f1_gnb, acc_gnb, f1_gnbsw, acc_gnbsw, f1_knn, acc_knn, f1_knnsw, acc_knnsw, f1_dt, acc_dt, f1_dtsw, acc_dtsw, f1_rfc, acc_rfc, f1_rfcsw, acc_rfcsw, f1_sw_cnn, acc_sw_cnn, f1_cnn, acc_cnn\n",
    "\n",
    "def get_tl_results(tl_cnn, resultsfold, xtest, ytest):\n",
    "    x1 = pad_sequences(np.array(xtest[\"Entities\"]))\n",
    "    x2 = pad_sequences(np.array(xtest[\"Dependencies\"]))\n",
    "    x3 = pad_sequences(np.array(xtest[\"Parts of Speech\"]))\n",
    "    x6 = pad_sequences(np.array(xtest[\"Secbert Outputs\"]))\n",
    "    #Result of TL\n",
    "    with torch.no_grad():\n",
    "        y_pred = tl_cnn.predict([x1,x2,x3,x6]).round()\n",
    "    \n",
    "    counter=0\n",
    "    resultsfold.loc[counter,\"TL\"] = \"\"\n",
    "    for i in y_pred:\n",
    "        resultsfold.loc[counter,\"TL\"] = i\n",
    "        counter+=1\n",
    "\n",
    "    f1_tl = f1_score(ytest, y_pred)\n",
    "    # f1_tl = sum(f1_tl)/len(f1_tl)\n",
    "     \n",
    "    acc_tl = accuracy_score(ytest, y_pred)\n",
    "\n",
    "    return resultsfold, f1_tl, acc_tl\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "------------------------------- Test 0 --------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------------------------------- Fold 1 --------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------------------------------- Training Transfer Learning model --------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/20\n",
      "435/435 [==============================] - 18s 30ms/step - loss: 0.3784 - accuracy: 0.8214\n",
      "Epoch 2/20\n",
      "435/435 [==============================] - 13s 30ms/step - loss: 0.2496 - accuracy: 0.8817\n",
      "Epoch 3/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.1695 - accuracy: 0.9225\n",
      "Epoch 4/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0946 - accuracy: 0.9609\n",
      "Epoch 5/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0406 - accuracy: 0.9860\n",
      "Epoch 6/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0286 - accuracy: 0.9931\n",
      "Epoch 7/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0151 - accuracy: 0.9952\n",
      "Epoch 8/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0157 - accuracy: 0.9953\n",
      "Epoch 9/20\n",
      "435/435 [==============================] - 13s 30ms/step - loss: 0.0069 - accuracy: 0.9974\n",
      "Epoch 10/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0038 - accuracy: 0.9979\n",
      "Epoch 11/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0045 - accuracy: 0.9979\n",
      "Epoch 12/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0037 - accuracy: 0.9979\n",
      "Epoch 13/20\n",
      "435/435 [==============================] - 13s 30ms/step - loss: 0.0031 - accuracy: 0.9979\n",
      "Epoch 14/20\n",
      "435/435 [==============================] - 13s 30ms/step - loss: 0.0262 - accuracy: 0.9915\n",
      "Epoch 15/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0310 - accuracy: 0.9903\n",
      "Epoch 16/20\n",
      "435/435 [==============================] - 13s 30ms/step - loss: 0.0128 - accuracy: 0.9962\n",
      "Epoch 17/20\n",
      "435/435 [==============================] - 13s 30ms/step - loss: 0.0195 - accuracy: 0.9963\n",
      "Epoch 18/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0190 - accuracy: 0.9962\n",
      "Epoch 19/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0054 - accuracy: 0.9970\n",
      "Epoch 20/20\n",
      "435/435 [==============================] - 13s 30ms/step - loss: 0.0030 - accuracy: 0.9979\n",
      "\n",
      "\n",
      "------------------------------- Training Entities, POS, Dependencies based models --------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\casil\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "174/174 [==============================] - 6s 26ms/step - loss: 0.4760 - accuracy: 0.7653\n",
      "Epoch 2/20\n",
      "174/174 [==============================] - 4s 26ms/step - loss: 0.4040 - accuracy: 0.8031\n",
      "Epoch 3/20\n",
      "174/174 [==============================] - 4s 25ms/step - loss: 0.3948 - accuracy: 0.8083\n",
      "Epoch 4/20\n",
      "174/174 [==============================] - 4s 25ms/step - loss: 0.3858 - accuracy: 0.8164\n",
      "Epoch 5/20\n",
      "174/174 [==============================] - 4s 25ms/step - loss: 0.3747 - accuracy: 0.8158\n",
      "Epoch 6/20\n",
      "174/174 [==============================] - 4s 25ms/step - loss: 0.3682 - accuracy: 0.8190\n",
      "Epoch 7/20\n",
      "174/174 [==============================] - 4s 25ms/step - loss: 0.3607 - accuracy: 0.8259\n",
      "Epoch 8/20\n",
      "174/174 [==============================] - 4s 25ms/step - loss: 0.3583 - accuracy: 0.8276\n",
      "Epoch 9/20\n",
      "174/174 [==============================] - 4s 25ms/step - loss: 0.3525 - accuracy: 0.8335\n",
      "Epoch 10/20\n",
      "174/174 [==============================] - 4s 25ms/step - loss: 0.3441 - accuracy: 0.8328\n",
      "Epoch 11/20\n",
      "174/174 [==============================] - 4s 26ms/step - loss: 0.3312 - accuracy: 0.8397\n",
      "Epoch 12/20\n",
      "174/174 [==============================] - 4s 25ms/step - loss: 0.3259 - accuracy: 0.8431\n",
      "Epoch 13/20\n",
      "174/174 [==============================] - 4s 25ms/step - loss: 0.3113 - accuracy: 0.8517\n",
      "Epoch 14/20\n",
      "174/174 [==============================] - 4s 25ms/step - loss: 0.2964 - accuracy: 0.8607\n",
      "Epoch 15/20\n",
      "174/174 [==============================] - 4s 25ms/step - loss: 0.2757 - accuracy: 0.8707\n",
      "Epoch 16/20\n",
      "174/174 [==============================] - 4s 26ms/step - loss: 0.2634 - accuracy: 0.8780\n",
      "Epoch 17/20\n",
      "174/174 [==============================] - 4s 25ms/step - loss: 0.2407 - accuracy: 0.8931\n",
      "Epoch 18/20\n",
      "174/174 [==============================] - 4s 25ms/step - loss: 0.2203 - accuracy: 0.9048\n",
      "Epoch 19/20\n",
      "174/174 [==============================] - 4s 25ms/step - loss: 0.2088 - accuracy: 0.9104\n",
      "Epoch 20/20\n",
      "174/174 [==============================] - 4s 25ms/step - loss: 0.1853 - accuracy: 0.9205\n",
      "\n",
      "\n",
      "------------------------------- Training Security Words and Similar Requirements based models --------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\casil\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "174/174 [==============================] - 5s 23ms/step - loss: 0.5652 - accuracy: 0.7242\n",
      "Epoch 2/20\n",
      "174/174 [==============================] - 4s 22ms/step - loss: 0.5158 - accuracy: 0.7729\n",
      "Epoch 3/20\n",
      "174/174 [==============================] - 4s 22ms/step - loss: 0.5007 - accuracy: 0.7777\n",
      "Epoch 4/20\n",
      "174/174 [==============================] - 4s 22ms/step - loss: 0.4870 - accuracy: 0.7821\n",
      "Epoch 5/20\n",
      "174/174 [==============================] - 4s 22ms/step - loss: 0.4775 - accuracy: 0.7882\n",
      "Epoch 6/20\n",
      "174/174 [==============================] - 4s 23ms/step - loss: 0.4645 - accuracy: 0.7965\n",
      "Epoch 7/20\n",
      "174/174 [==============================] - 4s 22ms/step - loss: 0.4505 - accuracy: 0.7992\n",
      "Epoch 8/20\n",
      "174/174 [==============================] - 4s 22ms/step - loss: 0.4382 - accuracy: 0.8096\n",
      "Epoch 9/20\n",
      "174/174 [==============================] - 4s 22ms/step - loss: 0.4245 - accuracy: 0.8128\n",
      "Epoch 10/20\n",
      "174/174 [==============================] - 4s 22ms/step - loss: 0.4086 - accuracy: 0.8205 1s\n",
      "Epoch 11/20\n",
      "174/174 [==============================] - 4s 22ms/step - loss: 0.4000 - accuracy: 0.8265\n",
      "Epoch 12/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.3912 - accuracy: 0.8282\n",
      "Epoch 13/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.3741 - accuracy: 0.8406\n",
      "Epoch 14/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.3636 - accuracy: 0.8443\n",
      "Epoch 15/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.3601 - accuracy: 0.8464\n",
      "Epoch 16/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.3518 - accuracy: 0.8498\n",
      "Epoch 17/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.3504 - accuracy: 0.8514\n",
      "Epoch 18/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.3448 - accuracy: 0.8519\n",
      "Epoch 19/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.3368 - accuracy: 0.8537\n",
      "Epoch 20/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.3372 - accuracy: 0.8559\n",
      "log\n",
      "0.7394778902503996\n",
      "0.7751724137931034\n",
      "log sw\n",
      "0.6813842482100239\n",
      "0.7544827586206897\n",
      "svm\n",
      "0.7922705314009661\n",
      "0.8220689655172414\n",
      "svm_sw\n",
      "0.7013888888888888\n",
      "0.7627586206896552\n",
      "Naive\n",
      "0.2077205882352941\n",
      "0.6036781609195402\n",
      "Naive sw\n",
      "0.22607110300820418\n",
      "0.6096551724137931\n",
      "knn\n",
      "0.7612687813021702\n",
      "0.8027586206896552\n",
      "knn sw\n",
      "0.6863207547169812\n",
      "0.7554022988505747\n",
      "dec tree\n",
      "0.7617521367521368\n",
      "0.7949425287356322\n",
      "dec tree sw\n",
      "0.6874221668742216\n",
      "0.7691954022988505\n",
      "ran forest\n",
      "0.7542627883650954\n",
      "0.774712643678161\n",
      "ran forest sw\n",
      "0.6986697513013302\n",
      "0.7604597701149425\n",
      "cnn sw\n",
      "0.6954407294832827\n",
      "0.7696551724137931\n",
      "cnn\n",
      "0.7735449735449735\n",
      "0.8032183908045977\n",
      "tl\n",
      "0.8478488982161595\n",
      "0.8666666666666667\n",
      "\n",
      "\n",
      "------------------------------- Fold 2 --------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------------------------------- Training Transfer Learning model --------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.5455 - accuracy: 0.8173\n",
      "Epoch 2/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.3916 - accuracy: 0.8985\n",
      "Epoch 3/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.2953 - accuracy: 0.9291\n",
      "Epoch 4/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.2372 - accuracy: 0.9480\n",
      "Epoch 5/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.1972 - accuracy: 0.9569\n",
      "Epoch 6/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.1647 - accuracy: 0.9648\n",
      "Epoch 7/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.1438 - accuracy: 0.9693\n",
      "Epoch 8/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.1339 - accuracy: 0.9698\n",
      "Epoch 9/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.1257 - accuracy: 0.9715\n",
      "Epoch 10/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.1220 - accuracy: 0.9704\n",
      "Epoch 11/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.1112 - accuracy: 0.9738\n",
      "Epoch 12/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.1036 - accuracy: 0.9759\n",
      "Epoch 13/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0989 - accuracy: 0.9783\n",
      "Epoch 14/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0953 - accuracy: 0.9786\n",
      "Epoch 15/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0930 - accuracy: 0.9788\n",
      "Epoch 16/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0912 - accuracy: 0.9791\n",
      "Epoch 17/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0909 - accuracy: 0.9791\n",
      "Epoch 18/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0908 - accuracy: 0.9791\n",
      "Epoch 19/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0907 - accuracy: 0.9791\n",
      "Epoch 20/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0907 - accuracy: 0.9791\n",
      "\n",
      "\n",
      "------------------------------- Training Entities, POS, Dependencies based models --------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\casil\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "174/174 [==============================] - 5s 25ms/step - loss: 0.5498 - accuracy: 0.7108\n",
      "Epoch 2/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.4669 - accuracy: 0.7976\n",
      "Epoch 3/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.4503 - accuracy: 0.8015\n",
      "Epoch 4/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.4354 - accuracy: 0.8076\n",
      "Epoch 5/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.4270 - accuracy: 0.8122\n",
      "Epoch 6/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.4179 - accuracy: 0.8164\n",
      "Epoch 7/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.4167 - accuracy: 0.8144\n",
      "Epoch 8/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.4100 - accuracy: 0.8155\n",
      "Epoch 9/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.4054 - accuracy: 0.8187\n",
      "Epoch 10/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.4009 - accuracy: 0.8206\n",
      "Epoch 11/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3922 - accuracy: 0.8229\n",
      "Epoch 12/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3870 - accuracy: 0.8250\n",
      "Epoch 13/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3791 - accuracy: 0.8253\n",
      "Epoch 14/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3729 - accuracy: 0.8318\n",
      "Epoch 15/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3604 - accuracy: 0.8278\n",
      "Epoch 16/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3481 - accuracy: 0.8339\n",
      "Epoch 17/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3387 - accuracy: 0.8373\n",
      "Epoch 18/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3324 - accuracy: 0.8399\n",
      "Epoch 19/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3336 - accuracy: 0.8427 0s - loss: 0.3273 - accu\n",
      "Epoch 20/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3204 - accuracy: 0.8435\n",
      "\n",
      "\n",
      "------------------------------- Training Security Words and Similar Requirements based models --------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\casil\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "174/174 [==============================] - 5s 22ms/step - loss: 0.5517 - accuracy: 0.7472\n",
      "Epoch 2/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.5158 - accuracy: 0.7711\n",
      "Epoch 3/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.5025 - accuracy: 0.7820\n",
      "Epoch 4/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.4898 - accuracy: 0.7841\n",
      "Epoch 5/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.4823 - accuracy: 0.7865\n",
      "Epoch 6/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.4729 - accuracy: 0.7914\n",
      "Epoch 7/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.4598 - accuracy: 0.7959\n",
      "Epoch 8/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.4533 - accuracy: 0.8027\n",
      "Epoch 9/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.4365 - accuracy: 0.8127 0s - loss: 0.4369 - accuracy\n",
      "Epoch 10/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.4270 - accuracy: 0.8169\n",
      "Epoch 11/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.4143 - accuracy: 0.8212\n",
      "Epoch 12/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.4038 - accuracy: 0.8257\n",
      "Epoch 13/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.3943 - accuracy: 0.8309\n",
      "Epoch 14/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.3841 - accuracy: 0.8359\n",
      "Epoch 15/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.3789 - accuracy: 0.8398\n",
      "Epoch 16/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.3678 - accuracy: 0.8447\n",
      "Epoch 17/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.3615 - accuracy: 0.8468\n",
      "Epoch 18/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.3567 - accuracy: 0.8496\n",
      "Epoch 19/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.3507 - accuracy: 0.8543\n",
      "Epoch 20/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.3460 - accuracy: 0.8559\n",
      "log\n",
      "0.7346938775510204\n",
      "0.7727690892364305\n",
      "log sw\n",
      "0.6591715976331362\n",
      "0.735050597976081\n",
      "svm\n",
      "0.7800524934383202\n",
      "0.8072677092916284\n",
      "svm_sw\n",
      "0.6906803887935964\n",
      "0.7511499540018399\n",
      "Naive\n",
      "0.29177268871925355\n",
      "0.6159153633854646\n",
      "Naive sw\n",
      "0.2630185348631951\n",
      "0.6159153633854646\n",
      "knn\n",
      "0.7565217391304349\n",
      "0.7939282428702852\n",
      "knn sw\n",
      "0.6759259259259258\n",
      "0.7424103035878565\n",
      "dec tree\n",
      "0.758438818565401\n",
      "0.7893284268629255\n",
      "dec tree sw\n",
      "0.6662576687116564\n",
      "0.749770009199632\n",
      "ran forest\n",
      "0.7340372046254399\n",
      "0.7566697332106715\n",
      "ran forest sw\n",
      "0.683066361556064\n",
      "0.7451701931922723\n",
      "cnn sw\n",
      "0.6791314837153197\n",
      "0.7552897884084636\n",
      "cnn\n",
      "0.7865731462925852\n",
      "0.8040478380864765\n",
      "tl\n",
      "0.8308374930671104\n",
      "0.859705611775529\n",
      "\n",
      "\n",
      "------------------------------- Fold 3 --------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------------------------------- Training Transfer Learning model --------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.3406 - accuracy: 0.8388\n",
      "Epoch 2/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.1761 - accuracy: 0.9240\n",
      "Epoch 3/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0682 - accuracy: 0.9759\n",
      "Epoch 4/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0253 - accuracy: 0.9920\n",
      "Epoch 5/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0239 - accuracy: 0.9929\n",
      "Epoch 6/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0162 - accuracy: 0.9946\n",
      "Epoch 7/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0191 - accuracy: 0.9936\n",
      "Epoch 8/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0167 - accuracy: 0.9946\n",
      "Epoch 9/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0074 - accuracy: 0.9968\n",
      "Epoch 10/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0100 - accuracy: 0.9970\n",
      "Epoch 11/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0091 - accuracy: 0.9970\n",
      "Epoch 12/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0074 - accuracy: 0.9968\n",
      "Epoch 13/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0060 - accuracy: 0.9975\n",
      "Epoch 14/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0030 - accuracy: 0.9983\n",
      "Epoch 15/20\n",
      "435/435 [==============================] - 13s 30ms/step - loss: 0.0032 - accuracy: 0.9985\n",
      "Epoch 16/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0027 - accuracy: 0.9985\n",
      "Epoch 17/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0025 - accuracy: 0.9985\n",
      "Epoch 18/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0026 - accuracy: 0.9985\n",
      "Epoch 19/20\n",
      "435/435 [==============================] - 13s 30ms/step - loss: 0.0061 - accuracy: 0.9984\n",
      "Epoch 20/20\n",
      "435/435 [==============================] - 13s 30ms/step - loss: 0.0161 - accuracy: 0.9945\n",
      "\n",
      "\n",
      "------------------------------- Training Entities, POS, Dependencies based models --------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\casil\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "174/174 [==============================] - 5s 25ms/step - loss: 0.5407 - accuracy: 0.7324\n",
      "Epoch 2/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.4742 - accuracy: 0.7951\n",
      "Epoch 3/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.4543 - accuracy: 0.8013\n",
      "Epoch 4/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.4425 - accuracy: 0.8057\n",
      "Epoch 5/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.4343 - accuracy: 0.8059\n",
      "Epoch 6/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.4117 - accuracy: 0.8083\n",
      "Epoch 7/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3865 - accuracy: 0.8112\n",
      "Epoch 8/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3758 - accuracy: 0.8161\n",
      "Epoch 9/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3684 - accuracy: 0.8172\n",
      "Epoch 10/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3626 - accuracy: 0.8214\n",
      "Epoch 11/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3544 - accuracy: 0.8248\n",
      "Epoch 12/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3482 - accuracy: 0.8265\n",
      "Epoch 13/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3423 - accuracy: 0.8294\n",
      "Epoch 14/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3352 - accuracy: 0.8305\n",
      "Epoch 15/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3236 - accuracy: 0.8376\n",
      "Epoch 16/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3135 - accuracy: 0.8433\n",
      "Epoch 17/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3062 - accuracy: 0.8494\n",
      "Epoch 18/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.2949 - accuracy: 0.8575\n",
      "Epoch 19/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.2880 - accuracy: 0.8650\n",
      "Epoch 20/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.2749 - accuracy: 0.8690\n",
      "\n",
      "\n",
      "------------------------------- Training Security Words and Similar Requirements based models --------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\casil\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "174/174 [==============================] - 4s 22ms/step - loss: 0.6110 - accuracy: 0.7101\n",
      "Epoch 2/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.5891 - accuracy: 0.7576\n",
      "Epoch 3/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.5702 - accuracy: 0.7665\n",
      "Epoch 4/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.5579 - accuracy: 0.7711\n",
      "Epoch 5/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.5476 - accuracy: 0.7747\n",
      "Epoch 6/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.5410 - accuracy: 0.7749\n",
      "Epoch 7/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.5380 - accuracy: 0.7762\n",
      "Epoch 8/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.6866 - accuracy: 0.5678\n",
      "Epoch 9/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.7429 - accuracy: 0.4269\n",
      "Epoch 10/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.7134 - accuracy: 0.4269\n",
      "Epoch 11/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.6984 - accuracy: 0.4269\n",
      "Epoch 12/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.6906 - accuracy: 0.5619\n",
      "Epoch 13/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.6864 - accuracy: 0.5732\n",
      "Epoch 14/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.6843 - accuracy: 0.5732\n",
      "Epoch 15/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.6832 - accuracy: 0.5732\n",
      "Epoch 16/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.6827 - accuracy: 0.5732\n",
      "Epoch 17/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.6825 - accuracy: 0.5732\n",
      "Epoch 18/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.6824 - accuracy: 0.5732\n",
      "Epoch 19/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.6824 - accuracy: 0.5732\n",
      "Epoch 20/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.6824 - accuracy: 0.5732\n",
      "log\n",
      "0.7473684210526316\n",
      "0.7792088316467342\n",
      "log sw\n",
      "0.676959619952494\n",
      "0.749770009199632\n",
      "svm\n",
      "0.7887771307570144\n",
      "0.8164673413063478\n",
      "svm_sw\n",
      "0.7127103888566454\n",
      "0.7723091076356946\n",
      "Naive\n",
      "0.23209428830462375\n",
      "0.610395584176633\n",
      "Naive sw\n",
      "0.25783348254252464\n",
      "0.6186752529898804\n",
      "knn\n",
      "0.7460055096418733\n",
      "0.7879484820607175\n",
      "knn sw\n",
      "0.6907817969661612\n",
      "0.7562097516099356\n",
      "dec tree\n",
      "0.7700592353257942\n",
      "0.8035878564857406\n",
      "dec tree sw\n",
      "0.6749999999999999\n",
      "0.7608095676172953\n",
      "ran forest\n",
      "0.7610887096774194\n",
      "0.7819687212511499\n",
      "ran forest sw\n",
      "0.6999419616947186\n",
      "0.7621895124195032\n",
      "cnn sw\n",
      "0.0\n",
      "0.5731370745170193\n",
      "cnn\n",
      "0.7912423625254583\n",
      "0.8114075436982521\n",
      "tl\n",
      "0.8366228070175438\n",
      "0.8629254829806807\n",
      "\n",
      "\n",
      "------------------------------- Fold 4 --------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------------------------------- Training Transfer Learning model --------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.4757 - accuracy: 0.7752\n",
      "Epoch 2/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.3930 - accuracy: 0.8387\n",
      "Epoch 3/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.3385 - accuracy: 0.8688\n",
      "Epoch 4/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.2842 - accuracy: 0.8955\n",
      "Epoch 5/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.2328 - accuracy: 0.9228\n",
      "Epoch 6/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.1956 - accuracy: 0.9380\n",
      "Epoch 7/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.1838 - accuracy: 0.9438\n",
      "Epoch 8/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.1640 - accuracy: 0.9506\n",
      "Epoch 9/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.1487 - accuracy: 0.9568\n",
      "Epoch 10/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.1466 - accuracy: 0.9564\n",
      "Epoch 11/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.1443 - accuracy: 0.9585\n",
      "Epoch 12/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.1306 - accuracy: 0.9631\n",
      "Epoch 13/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.1290 - accuracy: 0.9632\n",
      "Epoch 14/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.1298 - accuracy: 0.9629\n",
      "Epoch 15/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.1276 - accuracy: 0.9638\n",
      "Epoch 16/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.1265 - accuracy: 0.9641\n",
      "Epoch 17/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.1263 - accuracy: 0.9641\n",
      "Epoch 18/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.1262 - accuracy: 0.9641\n",
      "Epoch 19/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.1262 - accuracy: 0.9641\n",
      "Epoch 20/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.1267 - accuracy: 0.9639\n",
      "\n",
      "\n",
      "------------------------------- Training Entities, POS, Dependencies based models --------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\casil\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "174/174 [==============================] - 6s 24ms/step - loss: 0.4806 - accuracy: 0.7645\n",
      "Epoch 2/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.4052 - accuracy: 0.8067\n",
      "Epoch 3/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3963 - accuracy: 0.8091\n",
      "Epoch 4/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3865 - accuracy: 0.8145\n",
      "Epoch 5/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3801 - accuracy: 0.8169\n",
      "Epoch 6/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3752 - accuracy: 0.8183\n",
      "Epoch 7/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3649 - accuracy: 0.8250\n",
      "Epoch 8/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3619 - accuracy: 0.8255\n",
      "Epoch 9/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3570 - accuracy: 0.8278\n",
      "Epoch 10/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3508 - accuracy: 0.8293\n",
      "Epoch 11/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3460 - accuracy: 0.8349\n",
      "Epoch 12/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3444 - accuracy: 0.8355\n",
      "Epoch 13/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3370 - accuracy: 0.8379\n",
      "Epoch 14/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3272 - accuracy: 0.8440\n",
      "Epoch 15/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3156 - accuracy: 0.8517\n",
      "Epoch 16/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.3084 - accuracy: 0.8506\n",
      "Epoch 17/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.2933 - accuracy: 0.8608\n",
      "Epoch 18/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.2769 - accuracy: 0.8698\n",
      "Epoch 19/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.2554 - accuracy: 0.8848\n",
      "Epoch 20/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.2368 - accuracy: 0.8928\n",
      "\n",
      "\n",
      "------------------------------- Training Security Words and Similar Requirements based models --------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\casil\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "174/174 [==============================] - 4s 22ms/step - loss: 0.6120 - accuracy: 0.7262\n",
      "Epoch 2/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.5813 - accuracy: 0.7593\n",
      "Epoch 3/20\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 0.5682 - accuracy: 0.7675\n",
      "Epoch 4/20\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 0.5586 - accuracy: 0.7689\n",
      "Epoch 5/20\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 0.5506 - accuracy: 0.7708\n",
      "Epoch 6/20\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 0.5420 - accuracy: 0.7737\n",
      "Epoch 7/20\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 0.5358 - accuracy: 0.7743\n",
      "Epoch 8/20\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 0.5344 - accuracy: 0.7757\n",
      "Epoch 9/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.5288 - accuracy: 0.7775 0s -\n",
      "Epoch 10/20\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 0.5272 - accuracy: 0.7773\n",
      "Epoch 11/20\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 0.5204 - accuracy: 0.7810\n",
      "Epoch 12/20\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 0.5170 - accuracy: 0.7825\n",
      "Epoch 13/20\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 0.5116 - accuracy: 0.7821\n",
      "Epoch 14/20\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 0.5072 - accuracy: 0.7865\n",
      "Epoch 15/20\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 0.5038 - accuracy: 0.7894\n",
      "Epoch 16/20\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 0.5012 - accuracy: 0.7917\n",
      "Epoch 17/20\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 0.4965 - accuracy: 0.7933\n",
      "Epoch 18/20\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 0.4907 - accuracy: 0.7960\n",
      "Epoch 19/20\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 0.4793 - accuracy: 0.8000\n",
      "Epoch 20/20\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 0.4763 - accuracy: 0.8036\n",
      "log\n",
      "0.7418839808408727\n",
      "0.7769089236430543\n",
      "log sw\n",
      "0.6824364281490243\n",
      "0.7529898804047838\n",
      "svm\n",
      "0.7955625990491284\n",
      "0.8219871205151794\n",
      "svm_sw\n",
      "0.720554272517321\n",
      "0.7773689052437902\n",
      "Naive\n",
      "0.18975332068311193\n",
      "0.6071757129714811\n",
      "Naive sw\n",
      "0.2677304964539007\n",
      "0.6200551977920883\n",
      "knn\n",
      "0.7429519071310116\n",
      "0.7861085556577737\n",
      "knn sw\n",
      "0.7066282420749279\n",
      "0.765869365225391\n",
      "dec tree\n",
      "0.7706422018348622\n",
      "0.8045078196872125\n",
      "dec tree sw\n",
      "0.6984520123839009\n",
      "0.7759889604415824\n",
      "ran forest\n",
      "0.7591387080620932\n",
      "0.7787488500459981\n",
      "ran forest sw\n",
      "0.7031339031339031\n",
      "0.7603495860165593\n",
      "cnn sw\n",
      "0.7224157955865272\n",
      "0.7801287948482061\n",
      "cnn\n",
      "0.7765843179377014\n",
      "0.8086476540938362\n",
      "tl\n",
      "0.8409433015554441\n",
      "0.8541858325666973\n",
      "\n",
      "\n",
      "------------------------------- Fold 5 --------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------------------------------- Training Transfer Learning model --------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1/20\n",
      "435/435 [==============================] - 13s 28ms/step - loss: 0.4121 - accuracy: 0.8183\n",
      "Epoch 2/20\n",
      "435/435 [==============================] - 12s 28ms/step - loss: 0.2474 - accuracy: 0.8821\n",
      "Epoch 3/20\n",
      "435/435 [==============================] - 12s 28ms/step - loss: 0.1392 - accuracy: 0.9335\n",
      "Epoch 4/20\n",
      "435/435 [==============================] - 12s 28ms/step - loss: 0.0698 - accuracy: 0.9673\n",
      "Epoch 5/20\n",
      "435/435 [==============================] - 12s 28ms/step - loss: 0.0445 - accuracy: 0.9832\n",
      "Epoch 6/20\n",
      "435/435 [==============================] - 12s 28ms/step - loss: 0.0281 - accuracy: 0.9871\n",
      "Epoch 7/20\n",
      "435/435 [==============================] - 12s 28ms/step - loss: 0.0233 - accuracy: 0.9909\n",
      "Epoch 8/20\n",
      "435/435 [==============================] - 12s 28ms/step - loss: 0.0151 - accuracy: 0.9936\n",
      "Epoch 9/20\n",
      "435/435 [==============================] - 12s 28ms/step - loss: 0.0203 - accuracy: 0.9933\n",
      "Epoch 10/20\n",
      "435/435 [==============================] - 12s 28ms/step - loss: 0.0139 - accuracy: 0.9954\n",
      "Epoch 11/20\n",
      "435/435 [==============================] - 12s 28ms/step - loss: 0.0094 - accuracy: 0.9968\n",
      "Epoch 12/20\n",
      "435/435 [==============================] - 12s 28ms/step - loss: 0.0072 - accuracy: 0.9970\n",
      "Epoch 13/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0205 - accuracy: 0.9933\n",
      "Epoch 14/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0121 - accuracy: 0.9951\n",
      "Epoch 15/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0118 - accuracy: 0.9957\n",
      "Epoch 16/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0085 - accuracy: 0.9956\n",
      "Epoch 17/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0034 - accuracy: 0.9974\n",
      "Epoch 18/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0031 - accuracy: 0.9975\n",
      "Epoch 19/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0030 - accuracy: 0.9976\n",
      "Epoch 20/20\n",
      "435/435 [==============================] - 13s 29ms/step - loss: 0.0028 - accuracy: 0.9978\n",
      "\n",
      "\n",
      "------------------------------- Training Entities, POS, Dependencies based models --------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\casil\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "174/174 [==============================] - 5s 24ms/step - loss: 0.6908 - accuracy: 0.5688\n",
      "Epoch 2/20\n",
      "174/174 [==============================] - 4s 23ms/step - loss: 0.6855 - accuracy: 0.5732\n",
      "Epoch 3/20\n",
      "174/174 [==============================] - 4s 23ms/step - loss: 0.6836 - accuracy: 0.5732\n",
      "Epoch 4/20\n",
      "174/174 [==============================] - 4s 23ms/step - loss: 0.6828 - accuracy: 0.5732\n",
      "Epoch 5/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.6826 - accuracy: 0.5732\n",
      "Epoch 6/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.6825 - accuracy: 0.5732\n",
      "Epoch 7/20\n",
      "174/174 [==============================] - 4s 23ms/step - loss: 0.6824 - accuracy: 0.5732\n",
      "Epoch 8/20\n",
      "174/174 [==============================] - 4s 23ms/step - loss: 0.6824 - accuracy: 0.5732\n",
      "Epoch 9/20\n",
      "174/174 [==============================] - 4s 23ms/step - loss: 0.6824 - accuracy: 0.5732\n",
      "Epoch 10/20\n",
      "174/174 [==============================] - 4s 23ms/step - loss: 0.6824 - accuracy: 0.5732\n",
      "Epoch 11/20\n",
      "174/174 [==============================] - 4s 23ms/step - loss: 0.6824 - accuracy: 0.5732\n",
      "Epoch 12/20\n",
      "174/174 [==============================] - 4s 23ms/step - loss: 0.6824 - accuracy: 0.5732\n",
      "Epoch 13/20\n",
      "174/174 [==============================] - 4s 25ms/step - loss: 0.6824 - accuracy: 0.5732\n",
      "Epoch 14/20\n",
      "174/174 [==============================] - 4s 25ms/step - loss: 0.6824 - accuracy: 0.5732\n",
      "Epoch 15/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.6824 - accuracy: 0.5732\n",
      "Epoch 16/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.6824 - accuracy: 0.5732\n",
      "Epoch 17/20\n",
      "174/174 [==============================] - 4s 24ms/step - loss: 0.6824 - accuracy: 0.5732\n",
      "Epoch 18/20\n",
      "174/174 [==============================] - 4s 23ms/step - loss: 0.6824 - accuracy: 0.5732\n",
      "Epoch 19/20\n",
      "174/174 [==============================] - 4s 23ms/step - loss: 0.6824 - accuracy: 0.5732\n",
      "Epoch 20/20\n",
      "174/174 [==============================] - 4s 23ms/step - loss: 0.6824 - accuracy: 0.5732\n",
      "\n",
      "\n",
      "------------------------------- Training Security Words and Similar Requirements based models --------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\casil\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "174/174 [==============================] - 5s 23ms/step - loss: 0.6416 - accuracy: 0.7042\n",
      "Epoch 2/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.5791 - accuracy: 0.7699\n",
      "Epoch 3/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.5520 - accuracy: 0.7742\n",
      "Epoch 4/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.5278 - accuracy: 0.7804\n",
      "Epoch 5/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.5102 - accuracy: 0.7856\n",
      "Epoch 6/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.4942 - accuracy: 0.7914\n",
      "Epoch 7/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.4783 - accuracy: 0.7975\n",
      "Epoch 8/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.4615 - accuracy: 0.8065 0s -\n",
      "Epoch 9/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.4463 - accuracy: 0.8104\n",
      "Epoch 10/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.4296 - accuracy: 0.8138\n",
      "Epoch 11/20\n",
      "174/174 [==============================] - 4s 22ms/step - loss: 0.4076 - accuracy: 0.8270\n",
      "Epoch 12/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.3924 - accuracy: 0.8335 0s - loss:\n",
      "Epoch 13/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.3812 - accuracy: 0.8380\n",
      "Epoch 14/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.3697 - accuracy: 0.8422\n",
      "Epoch 15/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.3602 - accuracy: 0.8468\n",
      "Epoch 16/20\n",
      "174/174 [==============================] - 4s 22ms/step - loss: 0.3525 - accuracy: 0.8486 0s - loss: 0\n",
      "Epoch 17/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.3459 - accuracy: 0.8529\n",
      "Epoch 18/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.3468 - accuracy: 0.8536\n",
      "Epoch 19/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.3419 - accuracy: 0.8541\n",
      "Epoch 20/20\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.3401 - accuracy: 0.8549\n",
      "log\n",
      "0.7468085106382979\n",
      "0.781048758049678\n",
      "log sw\n",
      "0.6713697824808935\n",
      "0.7428702851885924\n",
      "svm\n",
      "0.7881040892193307\n",
      "0.8164673413063478\n",
      "svm_sw\n",
      "0.6967963386727689\n",
      "0.7562097516099356\n",
      "Naive\n",
      "0.12745098039215688\n",
      "0.5906163753449862\n",
      "Naive sw\n",
      "0.23869801084990958\n",
      "0.6126954921803128\n",
      "knn\n",
      "0.7398328690807799\n",
      "0.7851885924563018\n",
      "knn sw\n",
      "0.6879350348027842\n",
      "0.7525298988040479\n",
      "dec tree\n",
      "0.7436589314624932\n",
      "0.781508739650414\n",
      "dec tree sw\n",
      "0.6598855689764781\n",
      "0.7539098436062558\n",
      "ran forest\n",
      "0.7585863613738179\n",
      "0.7769089236430543\n",
      "ran forest sw\n",
      "0.6913861950941244\n",
      "0.7511499540018399\n",
      "cnn sw\n",
      "0.6868198307134219\n",
      "0.7617295308187673\n",
      "cnn\n",
      "0.0\n",
      "0.5731370745170193\n",
      "tl\n",
      "0.8309178743961353\n",
      "0.8551057957681693\n"
     ]
    }
   ],
   "source": [
    "accuracy = pd.DataFrame()\n",
    "f1score = pd.DataFrame()\n",
    "f1score_calc = pd.DataFrame()\n",
    "index=0\n",
    "\n",
    "accuracy.at[index,\"LR\"] = \"\"\n",
    "accuracy.at[index,\"LRSW\"] = \"\"\n",
    "accuracy.at[index,\"SVM\"] = \"\"\n",
    "accuracy.at[index,\"SVMSW\"] = \"\"\n",
    "accuracy.at[index,\"GNB\"] = \"\"\n",
    "accuracy.at[index,\"GNBSW\"] = \"\"\n",
    "accuracy.at[index,\"KNN\"] = \"\"\n",
    "accuracy.at[index,\"KNNSW\"] = \"\"\n",
    "accuracy.at[index,\"DT\"] = \"\"\n",
    "accuracy.at[index,\"DTSW\"] = \"\"\n",
    "accuracy.at[index,\"RFC\"] = \"\"\n",
    "accuracy.at[index,\"RFCSW\"] = \"\"\n",
    "accuracy.at[index,\"SW_CNN\"] = \"\"\n",
    "accuracy.at[index,\"CNN\"] = \"\"\n",
    "accuracy.at[index,\"TL\"] = \"\"\n",
    "\n",
    "f1score.at[index,\"LR\"] = \"\"\n",
    "f1score.at[index,\"LRSW\"] = \"\"\n",
    "f1score.at[index,\"SVM\"] = \"\"\n",
    "f1score.at[index,\"SVMSW\"] = \"\"\n",
    "f1score.at[index,\"GNB\"] = \"\"\n",
    "f1score.at[index,\"GNBSW\"] = \"\"\n",
    "f1score.at[index,\"KNN\"] = \"\"\n",
    "f1score.at[index,\"KNNSW\"] = \"\"\n",
    "f1score.at[index,\"DT\"] = \"\"\n",
    "f1score.at[index,\"DTSW\"] = \"\"\n",
    "f1score.at[index,\"RFC\"] = \"\"\n",
    "f1score.at[index,\"RFCSW\"] = \"\"\n",
    "f1score.at[index,\"SW_CNN\"] = \"\"\n",
    "f1score.at[index,\"CNN\"] = \"\"\n",
    "f1score.at[index,\"TL\"] = \"\"\n",
    "\n",
    "#----------------------------------------\n",
    "# Edit N to change the number of repetitions of the 5 fold cross validation\n",
    "#----------------------------------------\n",
    "N = 1\n",
    "\n",
    "for _ in itertools.repeat(None, N):\n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "    print('------------------------------- Test '+ str(index) + ' --------------------------------------')    \n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "    results = pd.DataFrame()\n",
    "    \n",
    "    f1_log_vector = []\n",
    "    acc_log_vector = []\n",
    "\n",
    "    f1_logsw_vector = []\n",
    "    acc_logsw_vector = []\n",
    "    \n",
    "    f1_svm_vector = []\n",
    "    acc_svm_vector = []\n",
    "\n",
    "    f1_svmsw_vector = []\n",
    "    acc_svmsw_vector = []\n",
    "    \n",
    "    f1_gnb_vector = []\n",
    "    acc_gnb_vector = []\n",
    "\n",
    "    f1_gnbsw_vector = []\n",
    "    acc_gnbsw_vector = []\n",
    "    \n",
    "    f1_knn_vector = []\n",
    "    acc_knn_vector = []\n",
    "\n",
    "    f1_knnsw_vector = []\n",
    "    acc_knnsw_vector = []\n",
    "    \n",
    "    f1_dt_vector = []\n",
    "    acc_dt_vector = []\n",
    "\n",
    "    f1_dtsw_vector = []\n",
    "    acc_dtsw_vector = []\n",
    "    \n",
    "    f1_rfc_vector = []\n",
    "    acc_rfc_vector = []\n",
    "\n",
    "    f1_rfcsw_vector = []\n",
    "    acc_rfcsw_vector = []\n",
    "\n",
    "    f1_sw_cnn_vector = []\n",
    "    acc_sw_cnn_vector = []\n",
    "\n",
    "    f1_disclo_cnn_vector = []\n",
    "    acc_cnn_vector = [] \n",
    "\n",
    "    f1_pd_vector = []\n",
    "    acc_pd_vector = []\n",
    "\n",
    "    counter = 0\n",
    "    while counter < 5:\n",
    "        \n",
    "        if counter==0:\n",
    "            x_train, x_test, y_train, y_test = create_fold_1(data)\n",
    "            print('\\n')\n",
    "            print('------------------------------- Fold '+ str(counter+1) + ' --------------------------------------')    \n",
    "            print('\\n')\n",
    "        elif counter==1:\n",
    "            x_train, x_test, y_train, y_test = create_fold_2(data)\n",
    "            print('\\n')\n",
    "            print('------------------------------- Fold '+ str(counter+1) + ' --------------------------------------')    \n",
    "            print('\\n')\n",
    "        elif counter==2:\n",
    "            x_train, x_test, y_train, y_test = create_fold_3(data)\n",
    "            print('\\n')\n",
    "            print('------------------------------- Fold '+ str(counter+1) + ' --------------------------------------')    \n",
    "            print('\\n')\n",
    "        elif counter==3:\n",
    "            x_train, x_test, y_train, y_test = create_fold_4(data)\n",
    "            print('\\n')\n",
    "            print('------------------------------- Fold '+ str(counter+1) + ' --------------------------------------')    \n",
    "            print('\\n')\n",
    "        elif counter==4:\n",
    "            x_train, x_test, y_train, y_test = create_fold_5(data)\n",
    "            print('\\n')\n",
    "            print('------------------------------- Fold '+ str(counter+1) + ' --------------------------------------')    \n",
    "            print('\\n')\n",
    "\n",
    "        K.clear_session()\n",
    "        torch.cuda.empty_cache()\n",
    "        K.set_session(K.tf.compat.v1.Session(config=cfg))\n",
    "        print('\\n')\n",
    "        print('------------------------------- Training Transfer Learning model --------------------------------------')    \n",
    "        print('\\n')\n",
    "        resultsfold = pd.DataFrame()\n",
    "        tl_cnn = create_TL_model(x_train, y_train)\n",
    "        resultsfold, f1_tl, acc_tl = get_tl_results(tl_cnn,resultsfold,x_test, y_test)\n",
    "        del tl_cnn\n",
    "        print('\\n')\n",
    "        print('------------------------------- Training Entities, POS, Dependencies based models --------------------------------------')    \n",
    "        print('\\n')\n",
    "        logreg, svm, gnb, knn, dt, rfc, cnn = create_models_EntPosDep(x_train, y_train)\n",
    "        print('\\n')\n",
    "        print('------------------------------- Training Security Words and Similar Requirements based models --------------------------------------')    \n",
    "        print('\\n')\n",
    "        logsw, svmsw, gnbsw, knnsw, dtsw, rfcsw, cnn_sw = create_model_SecWorSimReq(x_train, y_train)\n",
    "        \n",
    "        resultsfold, f1_log, acc_log, f1_logsw, acc_logsw, f1_svm, acc_svm, f1_svmsw, acc_svmsw, f1_gnb, acc_gnb, f1_gnbsw, acc_gnbsw, f1_knn, acc_knn, f1_knnsw, acc_knnsw, f1_dt, acc_dt, f1_dtsw, acc_dtsw, f1_rfc, acc_rfc, f1_rfcsw, acc_rfcsw, f1_sw_cnn, acc_sw_cnn, f1_cnn, acc_cnn = get_results(resultsfold, logreg, logsw, svm, svmsw, gnb, gnbsw, knn, knnsw, dt, dtsw, rfc, rfcsw, cnn_sw, cnn, x_test, y_test)\n",
    "        del cnn\n",
    "        del cnn_sw\n",
    "        \n",
    "\n",
    "        results = pd.concat([results, resultsfold], ignore_index=True)\n",
    "        \n",
    "        f1_log_vector.append(f1_log)\n",
    "        acc_log_vector.append(acc_log)\n",
    "        print(\"log\")\n",
    "        print(str(f1_log))\n",
    "        \n",
    "        print(str(acc_log))\n",
    "\n",
    "        print(\"log sw\")\n",
    "        print(str(f1_logsw))\n",
    "\n",
    "        print(str(acc_logsw))\n",
    "        print(\"svm\")\n",
    "        print(str(f1_svm))\n",
    "        \n",
    "        print(str(acc_svm))\n",
    "\n",
    "        print(\"svm_sw\")\n",
    "        print(str(f1_svmsw))\n",
    "\n",
    "        print(str(acc_svmsw))\n",
    "\n",
    "        print(\"Naive\")\n",
    "        print(str(f1_gnb))\n",
    "        \n",
    "        print(str(acc_gnb))\n",
    "\n",
    "        print(\"Naive sw\")\n",
    "        print(str(f1_gnbsw))\n",
    "\n",
    "        print(str(acc_gnbsw))\n",
    "        print(\"knn\")\n",
    "        print(str(f1_knn))\n",
    "        \n",
    "        print(str(acc_knn))\n",
    "\n",
    "        print(\"knn sw\")\n",
    "        print(str(f1_knnsw))\n",
    "\n",
    "        print(str(acc_knnsw))\n",
    "\n",
    "        print(\"dec tree\")\n",
    "        print(str(f1_dt))\n",
    "        \n",
    "        print(str(acc_dt))\n",
    "\n",
    "        print(\"dec tree sw\")\n",
    "        print(str(f1_dtsw))\n",
    "        \n",
    "        print(str(acc_dtsw))\n",
    "        print(\"ran forest\")\n",
    "        print(str(f1_rfc))\n",
    "        \n",
    "        print(str(acc_rfc))\n",
    "\n",
    "        print(\"ran forest sw\")\n",
    "        print(str(f1_rfcsw))\n",
    "\n",
    "        print(str(acc_rfcsw))\n",
    "\n",
    "        print(\"cnn sw\")\n",
    "        print(str(f1_sw_cnn))\n",
    "        print(str(acc_sw_cnn))\n",
    "\n",
    "        print(\"cnn\")\n",
    "        print(str(f1_cnn))\n",
    "        \n",
    "        print(str(acc_cnn)) \n",
    "\n",
    "        print(\"tl\")\n",
    "        print(str(f1_tl))\n",
    "        \n",
    "        print(str(acc_tl))\n",
    "  \n",
    "        f1_logsw_vector.append(f1_logsw)\n",
    "        acc_logsw_vector.append(acc_logsw)\n",
    "    \n",
    "        f1_svm_vector.append(f1_svm)\n",
    "        acc_svm_vector.append(acc_svm)\n",
    "\n",
    "        f1_svmsw_vector.append(f1_svmsw)\n",
    "        acc_svmsw_vector.append(acc_svmsw)\n",
    "    \n",
    "        f1_gnb_vector.append(f1_gnb)\n",
    "        acc_gnb_vector.append(acc_gnb)\n",
    "\n",
    "        f1_gnbsw_vector.append(f1_gnbsw)\n",
    "        acc_gnbsw_vector.append(acc_gnbsw)\n",
    "    \n",
    "        f1_knn_vector.append(f1_knn)\n",
    "        acc_knn_vector.append(acc_knn)\n",
    "\n",
    "        f1_knnsw_vector.append(f1_knnsw)\n",
    "        acc_knnsw_vector.append(acc_knnsw)\n",
    "    \n",
    "        f1_dt_vector.append(f1_dt)\n",
    "        acc_dt_vector.append(acc_dt)\n",
    "\n",
    "        f1_dtsw_vector.append(f1_dtsw)\n",
    "        acc_dtsw_vector.append(acc_dtsw)\n",
    "    \n",
    "        f1_rfc_vector.append(f1_rfc)\n",
    "        acc_rfc_vector.append(acc_rfc)\n",
    "\n",
    "        f1_rfcsw_vector.append(f1_rfcsw)\n",
    "        acc_rfcsw_vector.append(acc_rfcsw)\n",
    "    \n",
    "        f1_sw_cnn_vector.append(f1_sw_cnn)\n",
    "        acc_sw_cnn_vector.append(acc_sw_cnn)\n",
    "\n",
    "        f1_disclo_cnn_vector.append(f1_cnn)\n",
    "        acc_cnn_vector.append(acc_cnn) \n",
    "\n",
    "        f1_pd_vector.append(f1_tl)\n",
    "        acc_pd_vector.append(acc_tl)\n",
    "        \n",
    "        counter +=1\n",
    "    \n",
    "    results.to_excel('Results_Test_'+str(index+1)+'.xlsx',index=False)\n",
    "    \n",
    "    accuracy.at[index,\"LR\"] = sum(acc_log_vector)/len(acc_log_vector)\n",
    "    accuracy.at[index,\"LRSW\"] = sum(acc_logsw_vector)/len(acc_logsw_vector)\n",
    "    \n",
    "    accuracy.at[index,\"SVM\"] = sum(acc_svm_vector)/len(acc_svm_vector)\n",
    "    accuracy.at[index,\"SVMSW\"] = sum(acc_svmsw_vector)/len(acc_svmsw_vector)\n",
    "    \n",
    "    accuracy.at[index,\"GNB\"] = sum(acc_gnb_vector)/len(acc_gnb_vector)\n",
    "    accuracy.at[index,\"GNBSW\"] = sum(acc_gnbsw_vector)/len(acc_gnbsw_vector)\n",
    "    \n",
    "    accuracy.at[index,\"KNN\"] = sum(acc_knn_vector)/len(acc_knn_vector)\n",
    "    accuracy.at[index,\"KNNSW\"] = sum(acc_knnsw_vector)/len(acc_knnsw_vector)\n",
    "    \n",
    "    accuracy.at[index,\"DT\"] = sum(acc_dt_vector)/len(acc_dt_vector)\n",
    "    accuracy.at[index,\"DTSW\"] = sum(acc_dtsw_vector)/len(acc_dtsw_vector)\n",
    "    \n",
    "    accuracy.at[index,\"RFC\"] = sum(acc_rfc_vector)/len(acc_rfc_vector)\n",
    "    accuracy.at[index,\"RFCSW\"] = sum(acc_rfcsw_vector)/len(acc_rfcsw_vector)\n",
    "    \n",
    "    accuracy.at[index,\"SW_CNN\"] = sum(acc_sw_cnn_vector)/len(acc_sw_cnn_vector)\n",
    "    accuracy.at[index,\"CNN\"] = sum(acc_cnn_vector)/len(acc_cnn_vector)\n",
    "    accuracy.at[index,\"TL\"] = sum(acc_pd_vector)/len(acc_pd_vector)\n",
    "    \n",
    "    f1score.at[index,\"LR\"] = sum(f1_log_vector)/len(f1_log_vector)\n",
    "    f1score.at[index,\"LRSW\"] = sum(f1_logsw_vector)/len(f1_logsw_vector)\n",
    "    \n",
    "    f1score.at[index,\"SVM\"] = sum(f1_svm_vector)/len(f1_svm_vector)\n",
    "    f1score.at[index,\"SVMSW\"] = sum(f1_svmsw_vector)/len(f1_svmsw_vector)\n",
    "    \n",
    "    f1score.at[index,\"GNB\"] = sum(f1_gnb_vector)/len(f1_gnb_vector)\n",
    "    f1score.at[index,\"GNBSW\"] = sum(f1_gnbsw_vector)/len(f1_gnbsw_vector)\n",
    "    \n",
    "    f1score.at[index,\"KNN\"] = sum(f1_knn_vector)/len(f1_knn_vector)\n",
    "    f1score.at[index,\"KNNSW\"] = sum(f1_knnsw_vector)/len(f1_knnsw_vector)\n",
    "    \n",
    "    f1score.at[index,\"DT\"] = sum(f1_dt_vector)/len(f1_dt_vector)\n",
    "    f1score.at[index,\"DTSW\"] = sum(f1_dtsw_vector)/len(f1_dtsw_vector)\n",
    "    \n",
    "    f1score.at[index,\"RFC\"] = sum(f1_rfc_vector)/len(f1_rfc_vector)\n",
    "    f1score.at[index,\"RFCSW\"] = sum(f1_rfcsw_vector)/len(f1_rfcsw_vector)\n",
    "    \n",
    "    f1score.at[index,\"SW_CNN\"] = sum(f1_sw_cnn_vector)/len(f1_sw_cnn_vector)\n",
    "    f1score.at[index,\"CNN\"] = sum(f1_disclo_cnn_vector)/len(f1_disclo_cnn_vector)\n",
    "    f1score.at[index,\"TL\"] = sum(f1_pd_vector)/len(f1_pd_vector)\n",
    "    \n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>LRSW</th>\n",
       "      <th>SVM</th>\n",
       "      <th>SVMSW</th>\n",
       "      <th>GNB</th>\n",
       "      <th>GNBSW</th>\n",
       "      <th>KNN</th>\n",
       "      <th>KNNSW</th>\n",
       "      <th>DT</th>\n",
       "      <th>DTSW</th>\n",
       "      <th>RFC</th>\n",
       "      <th>RFCSW</th>\n",
       "      <th>SW_CNN</th>\n",
       "      <th>CNN</th>\n",
       "      <th>TL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.742047</td>\n",
       "      <td>0.674264</td>\n",
       "      <td>0.788953</td>\n",
       "      <td>0.704426</td>\n",
       "      <td>0.209758</td>\n",
       "      <td>0.25067</td>\n",
       "      <td>0.749316</td>\n",
       "      <td>0.689518</td>\n",
       "      <td>0.76091</td>\n",
       "      <td>0.677403</td>\n",
       "      <td>0.753423</td>\n",
       "      <td>0.69524</td>\n",
       "      <td>0.556762</td>\n",
       "      <td>0.625589</td>\n",
       "      <td>0.837434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         LR      LRSW       SVM     SVMSW       GNB    GNBSW       KNN  \\\n",
       "0  0.742047  0.674264  0.788953  0.704426  0.209758  0.25067  0.749316   \n",
       "\n",
       "      KNNSW       DT      DTSW       RFC    RFCSW    SW_CNN       CNN  \\\n",
       "0  0.689518  0.76091  0.677403  0.753423  0.69524  0.556762  0.625589   \n",
       "\n",
       "         TL  \n",
       "0  0.837434  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#.to_excel(\"F1_Results.xlsx\")\n",
    "f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>LRSW</th>\n",
       "      <th>SVM</th>\n",
       "      <th>SVMSW</th>\n",
       "      <th>GNB</th>\n",
       "      <th>GNBSW</th>\n",
       "      <th>KNN</th>\n",
       "      <th>KNNSW</th>\n",
       "      <th>DT</th>\n",
       "      <th>DTSW</th>\n",
       "      <th>RFC</th>\n",
       "      <th>RFCSW</th>\n",
       "      <th>SW_CNN</th>\n",
       "      <th>CNN</th>\n",
       "      <th>TL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.777022</td>\n",
       "      <td>0.747033</td>\n",
       "      <td>0.816852</td>\n",
       "      <td>0.763959</td>\n",
       "      <td>0.605556</td>\n",
       "      <td>0.615399</td>\n",
       "      <td>0.791186</td>\n",
       "      <td>0.754484</td>\n",
       "      <td>0.794775</td>\n",
       "      <td>0.761935</td>\n",
       "      <td>0.773802</td>\n",
       "      <td>0.755864</td>\n",
       "      <td>0.727988</td>\n",
       "      <td>0.760092</td>\n",
       "      <td>0.859718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         LR      LRSW       SVM     SVMSW       GNB     GNBSW       KNN  \\\n",
       "0  0.777022  0.747033  0.816852  0.763959  0.605556  0.615399  0.791186   \n",
       "\n",
       "      KNNSW        DT      DTSW       RFC     RFCSW    SW_CNN       CNN  \\\n",
       "0  0.754484  0.794775  0.761935  0.773802  0.755864  0.727988  0.760092   \n",
       "\n",
       "         TL  \n",
       "0  0.859718  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36d6318a3d5c5964411ed9f3584e3f4ce9edc113893149ad40a0532cf5bc1f9e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
